---
marp: true
title: 24 Jun 2024
size: 16:9
math: mathjax
theme: am_dark
paginate: true
headingDivider: [2,3]
# backgroundImage: url('https://marp.app/assets/hero-background.svg')
---

<!-- _class: cover_b -->
<!-- _header: "" -->
<!-- _footer: "" -->
<!-- _paginate: "" -->

# Robot Perception and Control

###### Reinforcement Learning

Last updated: Jul / 25 /2024
Kashu Yamazaki
kyamazak@andrew.cmu.edu

## Application of Reinforcement Learning

<!-- _class: trans -->
<!-- _footer: "" -->
<!-- _paginate: "" -->

## Game Agents

![#center w:1000](https://www.researchgate.net/publication/353294015/figure/fig4/AS:1046198521716737@1626444573305/Popular-video-game-platforms-for-Reinforcement-Learning-research-summarized-from-59.ppm)

## Alignment of LLMs


## Robot Control


## Basics of Reinforcement Learning

<!-- _class: trans -->
<!-- _footer: "" -->
<!-- _paginate: "" -->

## Problem Setup

**Markov decision process (MDP)**


**Partially Observable MDP (POMDP)**

When the agent does not have direct access to the state $s_t$ but can observe the environment via observation $o_t$ that is determined by the previous action $a_{t-1}$ and the state $s_t$. 

## Reward and Return

**Reward** $r_{t+1}$: the evaluation of an action $a_t$ to the state $s_t$.

**Return** $R_t$: the evaluation of future rewards $\{r_{t+1}, r_{t+2}, \dots\}$

We usually apply a discount factor ($0 \leq \gamma \leq 1$) 
$$
R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots = \sum_{k=0}^{\infty}{\gamma^k r_{t+k+1}}
$$