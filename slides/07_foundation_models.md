---
marp: true
title: 24 Jun 2024
size: 16:9
math: mathjax
theme: am_dark
paginate: true
headingDivider: [2,3]
footer: Kashu Yamazaki, 2024
# backgroundImage: url('https://marp.app/assets/hero-background.svg')
---

<!-- _class: cover_b -->
<!-- _header: "" -->
<!-- _footer: "" -->
<!-- _paginate: "" -->
<!-- _backgroundImage: url('https://marp.app/assets/hero-background.svg') -->

# Robot Perception and Control

###### Foundation Models for Robotics

Last updated: Jul / 25 /2024
Kashu Yamazaki
kyamazak@andrew.cmu.edu


##  Datasets

<!-- _class: trans -->
<!-- _footer: "" -->
<!-- _paginate: "" -->

## BridgeData V2 [arxiv](https://arxiv.org/abs/2308.12952) [page](https://rail-berkeley.github.io/bridgedata/)

## Open X-Embodiment

![#center](https://robotics-transformer-x.github.io/img/overview.png)

A large-scale collection of datasets from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). The dataset follows the `RLDS` format.

## DROID

DROID uses the same hardware setup using Panda robot across all 13 institutions to streamline data collection while maximizing portability and flexibility.

![bg right:35% w:90%](https://droid-dataset.github.io/figures/droid_setup.png)
![bg vertical right:35% w:90%](https://droid-dataset.github.io/figures/droid_viewpoint_distribution.png)

##  Models

<!-- _class: trans -->
<!-- _footer: "" -->
<!-- _paginate: "" -->

## RT1



## RT2



![bg right:40% 100%](img/rt2_arch.png)

## RT-X

## Octo [arxiv](https://arxiv.org/abs/2405.12213)

![#center w:800](https://octo-models.github.io/architecture.jpg)

## OpenVLA [arxiv](https://arxiv.org/abs/2406.09246)

![#center w:800](https://openvla.github.io/static/images/openvla_model.jpg)